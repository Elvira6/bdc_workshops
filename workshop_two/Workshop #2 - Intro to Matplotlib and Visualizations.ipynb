{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop #2 - Simple Data Transformations\n",
    "\n",
    "This week we'll look on visualizing our data. Although numbers can tell us a satisfying story, a visual of our data will allow us to easily identify trends (and will look amazing on a presentation \\*hint\\* \\*hint\\*).\n",
    "\n",
    "If you came from the R version of this workshop and wish to use ggplot's tools, there is an excellent Python implimentation of ggplot which uses the same functions: http://ggplot.yhathq.com/how-it-works.html.\n",
    "\n",
    "## A few ground rules\n",
    "\n",
    " - Remember to run every cell.\n",
    "     - Parts of this workshop won't work if this condition isn't met.\n",
    " - Please don't change my asserts.\n",
    "     - If you're receiving an incorrect answer please don't change the assert answer just to get it right. You            won't learn anything and will probably fail the rest of the tutorial. Feel free to message slack on the #workshops channel if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#Don't mind this. I'm just trying to double check you work :)\n",
    "def assertAns(condition, fail_str, suc_str):\n",
    "    assert condition, fail_str\n",
    "    print(suc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Linear algebra\n",
    "import requests as req #Python's http library\n",
    "import re #Python's Regex library\n",
    "import pandas as pd #Python's data manipulation library\n",
    "import matplotlib.pyplot as plt #Python's graphing library\n",
    "import seaborn as sns #A graphing library for heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by importing the dataframe from last week's workshop (recall the pickle file)! The dataframe should saved in this path: `\"../saved_dataframes/workshop1RainfallDF\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeHourRainfallDF = pd.read_pickle(<FILL IN>)\n",
    "timeHourRainfallDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatplotLib Basics\n",
    "\n",
    "MatplotLib is a useful library to view your data as a graph. This is a pretty expansive library and includes basically all types of graphs known to man (and most definitely everything you've learned in a stats course).\n",
    "\n",
    "Lets start by plotting __the amount of rain by the time of day.__\n",
    "\n",
    "We'll start off by making two arrays, one to represent the x-values and the other the y-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = timeHourRainfallDF[\"time\"].unique()\n",
    "y = timeHourRainfallDF[\"rainfall\"].values #The values() function will return all the values of the column as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets print out `x` and `y` to see how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(<FILL IN>)\n",
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm that's annoying. Notice how the values of `x` are all strings. Unfortunately Matplotlib doesn't really like to use non-primitive types as a graph axis. We'll have to represent these strings with a continuous variable for Matplotlib to plot the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get the values of x in the form of a list.\n",
    "uniqueTimes = x\n",
    "uniqueTimes = list(uniqueTimes)\n",
    "print(\"type of the variable x: \"+ str(type(x)))\n",
    "print(\"type of the variable uniqueTimes: \"+ str(type(uniqueTimes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I am looping through each unique time and using its index as a unique continuous variable.\n",
    "for index,row in timeHourRainfallDF.iterrows():\n",
    "    timeHourRainfallDF.set_value(index,'timeIndex', uniqueTimes.index(row[\"time\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeHourRainfallDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets overwrite our `x` variable with the _values_ of the _timeIndex_ column from the `timeHourRainfallDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets construct our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm that's a bit small. lets increase it by inserting `plt.rcParams[\"figure.figsize\"] = \"18, 13\"` before we tweak the font."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>\n",
    "font = {'size': 16} #You can tweak around with the font params to make them prettier.\n",
    "plt.rc('font', **font) #Note: You only need to modify font global variables once and your changes will persist.\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set lables for the table. Set the _title_ for the grapph as `\"24-Hour Rainfall\"`, the _xlabel_ as `\"Time of Day\"`, and the _ylabel_ as `\"Amount of Rainfall (mm)\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(<FILL IN>)\n",
    "plt.xlabel(<FILL IN>)\n",
    "plt.ylabel(<FILL IN>)\n",
    "plt.rcParams[\"figure.figsize\"] = \"18, 13\"\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save your graphs by right-clicking on the image and select _save as_. You can also preview them as larger images by right-clicking and opening your graph in a new tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be honest I wasn't expecting spikes as clear as the ones shown above. And that's the beauty of graphing! It allows you to visually see patterns you might miss. Here you can clearly see when it rains the most and least in Toronto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my favourite things to do is to plot two different variables against each other and look at their correlation (http://www.statisticshowto.com/what-is-correlation/). This will allow us to see which pairs of variables seem to have the most correlation with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets define a function to make correlation matrices!\n",
    "def plot_correlation_map( df ):\n",
    "    corr = df.corr()\n",
    "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n",
    "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={ 'shrink' : .9 }, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = { 'fontsize' : 12 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you pass the `timeHourRainfallDF` into the `plot_correlation_map()` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_correlation_map(<FILL IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, most variables in our dataset doesn't really correlate with one another. Afterall, it's hard to see how rainfall will influence the time index. You will notice how every variable is 100% correlated with itself (which makes sense). To generate this correlation graph we were calculating the covariance against each variable. Something you can learn more about here: http://www.wikihow.com/Calculate-Covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some prerequsite knowledge on how to plot and graph in Python, lets try to visualize something more challenging. I want us to __plot downpours of rain on top of a map of Toronto.__\n",
    "\n",
    "Lets start by importing the data we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/rainfall201701.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/rainfall201702.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/rainfall201703.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/rainfall201704.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/rainfall201705.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/rainfall201706.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/sites201701.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/sites201702.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/sites201703.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/sites201704.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/sites201705.csv\"))\n",
    "datalist.append(pd.read_csv(\"../2017_rainfall_data/sites201706.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see what kind of data we've imported into our datalist. Run the `head()` function onto the _0th_ index of `datalist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist[5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist[6].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist[11].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we imported the packages together, we know that the data from indexes 0-5 contains the rainfall data and that indexes 6-11 contains the geographical data.<br>_Note: We could also have inferred this from the code we used to import our data, but it's always good to check!_\n",
    "\n",
    "Lets combine the dataframes 0-5 into one giant _rainfallDF_ and dataframes 6-11 into one giant _geoDataDF_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfallDF = datalist[0].append(datalist[1]).append(datalist[2]).append(datalist[3]).append(datalist[4]).append(datalist[5])\n",
    "geoDataDF = datalist[6].append(datalist[7]).append(datalist[8]).append(datalist[9]).append(datalist[10]).append(datalist[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how large each DF is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of rainfallDF: \" + str(len(rainfallDF)))\n",
    "print(\"length of geoDataDF: \" + str(len(geoDataDF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these two dataframes describes different datasets (because the number of rows don't match up).\n",
    "\n",
    "Yet if we looked at the _README_ for the dataset (https://goo.gl/gBYrb4), we'll realize that the values within `rainfallDF` describes the rain guage readings while the `geoDataDF` describes the locations of those rain guage stations.\n",
    "\n",
    "So lets print the number of stations that have a unique _name_ from `rainfallDF` and `geoDataDF` just to make sure that they both describe the same set of stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of unique station names in rainfallDF: \" + str(len(rainfallDF[\"name\"].unique())))\n",
    "print(\"length of unique ids in geoDataDF: \" + str(len(geoDataDF[\"name\"].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is one more station location described in the geoDataDF than in the rainfallDF. This probably means that a new station was built but no readings have been recorded yet. Nonetheless, we have enough station locations to correspond each rainfall reading with the location from which it was read from. You may have noticed that every name in the geoDataDF describes the location of one geo station. Each station has many readings and their readings are recorded in the rainfallDF.\n",
    "\n",
    "Now that we have a better understanding of the data within our two datasets, lets try to match up every rainfall reading with the _lat lon_ of every station. This will give each rainfall reading a coordinate which we can use to plot on a map!\n",
    "\n",
    "To match each rain guage reading with its corresponding station we'll have the `merge()` the two datasets on the _name_ column. This means every row in `rainfallDF` will have 2 extra attributes that describes its corresponding _lat lon_ location. If you're using multiple dataframes in your analysis remember that this function exists!\n",
    "\n",
    "We'll use the Pandas `merge()` function to join the dataframes by the _name_ column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoRainfallDFPoorColumns = rainfallDF.merge(geoDataDF, left_on='name', right_on='name', how='outer')\n",
    "geoRainfallDFPoorColumns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geoRainfallDFPoorColumns.drop(['id_x'], axis = 1, inplace = True, errors = 'ignore')\n",
    "# errors = 'ignore' means that if the id_x column doesn't exist this won't error.\n",
    "geoRainfallDFPoorColumns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're going to use this dataframe for future workshops, lets fix up the columns, specifically id_y. Lets rename it to just id and move this column to the left most index of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This makes it so that if your decide to re-run this cell then the columns will still be \"disorganized\".\n",
    "geoRainfallDF = geoRainfallDFPoorColumns\n",
    "\n",
    "#Lets set the name of the columns.\n",
    "geoRainfallDF.columns = ['name', 'date', 'rainfall', 'id', 'longitude', 'latitude']\n",
    "\n",
    "#Lets re-roder the id column. Doesn't it look familiar?\n",
    "geoRainfallDF = geoRainfallDF[['id', 'name', 'date', 'rainfall', 'longitude', 'latitude']]\n",
    "geoRainfallDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets save this dataframe for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoRainfallDF.to_pickle(\"../saved_dataframes/geoRainfallDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So without further adeau, lets start and precipitation data ontop of a map of Toronto!\n",
    "\n",
    "Although Python has many great mapping libraries ranging from Plotly, gmaps, or even Basemap, we'll be using Folium because it merges the best of asthetics, simplicity, and utility.\n",
    "\n",
    "Folium is a project that builds ontop of the fantastic mapping library Leaflet. I personally think it's prettier and easier to setup than google maps.\n",
    "\n",
    "Folium: https://github.com/python-visualization/folium\n",
    "\n",
    "Leaflet: https://github.com/Leaflet/Leaflet\n",
    "\n",
    "So lets download Folium!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium #The exclamation mark makes the current line Bash code\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[43.6532, -79.3832], zoom_start=12) #For those of you wondering those are Toronto's lat lons.\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty nice! You can also place points on a map. Try clicking on them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.Marker([43.6426, -79.3871], popup='<i>CN Tower</i>').add_to(m)\n",
    "folium.Marker([43.6677, -79.3948], popup='<b>Royal Ontario Museum</b>').add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we cannot place pins down for every single reading (because all we would see are pins). Lets instead make a heatmap of all the readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(geoRainfallDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa that's a lot of rows! My computer would most likely explode if I displayed 10 million points on a map. Lets try to sample 1000 rows of this data to make it easier on our computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallerDF = geoRainfallDF.sample(1000) #sample() selects a RANDOM subset each time it's run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side Note:\n",
    "\n",
    "Running machine learning algorithms on an entire dataset can sometimes take a long time. If you're tweaking your model, you should run the algorithm on a _subset_ of dataset to quickly gauge the success of your model. This is because machine learning algorithms will usually see diminishing returns when taking in more and more data. So it's best to find the best algorithm/parameters before running it on your full dataset. Anyway, lets plot this on a heatmap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "m2 = folium.Map(location=[43.6532, -79.3832], zoom_start=12)\n",
    "latlons = smallerDF[[\"latitude\",\"longitude\"]].values\n",
    "latlons = latlons.tolist() #The library requires this to be a python list.\n",
    "HeatMap(latlons).add_to(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: it might take a while to display it on a map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm notice how the points are clustering to specific spots on the map? Since we are displaying the readings for 100 or so locations, all the points of each location will lie directly on top of each other... Not very informative.\n",
    "\n",
    "Most heatmap libraries will display  the heat of every location based each location's \"hotness\" in relation to the rest of the image. In essence, the larger the percentage of heat, the \"hotter\" an area appears. With this in mind, try zooming into the map on a cluster of points. You can see how certain blobs turn lighter and maybe some will even turn red. This is because the heatmap will disregard the irrelevant plots (because you zoomed  in). Now each individual reading will weigh more and thus, some might turn brighter. If you zoom in and look at only one point, it'll most certainly turn bright red because all of the points visible on screen is on that point.\n",
    "\n",
    "In your own analyses, if your points are constantly red, then you might want to sample a smaller subset of your dataset to make the heatmap more sensitive to variations (counts of points in an area)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to show you how to display points on a map as a circle instead of \"pins\". Lets plot every _lat lon_ of every station as a blue dot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets read the _lat lon_ values of each station from the `sites201706.csv` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationLocations = pd.read_csv(\"../2017_rainfall_data/sites201706.csv\")\n",
    "stationLocations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = folium.Map(location=[43.6532, -79.3832], zoom_start=12)\n",
    "for index,row in stationLocations.iterrows():\n",
    "    folium.CircleMarker((row[\"latitude\"],row[\"longitude\"]),radius=1,color='red',fill=True).add_to(m3)\n",
    "#Note: You can change params of the circle like so:\n",
    "\n",
    "#folium.CircleMarker(latlons,\n",
    "#                    radius=500,\n",
    "#                    popup='Station',\n",
    "#                    color='#3186cc',\n",
    "#                    fill_color='#3186cc',\n",
    "#                   ).add_to(m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets put the heatmap on the markers to make sure we're not missing any stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HeatMap(latlons).add_to(m3)\n",
    "m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Follium is an interactive map we cannot right click and hit \"save as\" to save the image. Instead we have to call the `save(\"name_of_file.html\")` function of our m3 object to save our image. Try implementing that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's it! Remember to always graph when you aren't sure how to procceed or have doubts. More often than not your graphs will tell you something numbers can't show. So graph away because with just a few lines of code you can generate captivating visuals and make your analysis more appealing. I hope you'll use your newfound graphing skills to encover new findings and we'll see you next week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
