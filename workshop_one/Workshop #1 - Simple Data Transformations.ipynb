{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop #1 - Simple Data Transformations\n",
    "\n",
    "Welcome to the workshops! Every week we'll cover a new datascience topic in Python to help familiarize your team for the competition! Every workshop will teach you the bare essentials of data science. There is no fluff. Please complete every workshop to give yourself a full grasp of the concept. If you have any trouble with the workshops please message the #workshops channel on Slack. Slack is a place for everybody to learn so if you know how to answer something feel free to respond to that channel!\n",
    "\n",
    "This week will cover dataframe manipulation and we'll be working with the Toronto *Rain Gauge Locations and Precipitation* dataset.\n",
    "\n",
    "This is large collection of rainfall measurements taken over the past 3 years. (Link to the complete dataset: https://goo.gl/gBYrb4). I have already downloaded the data for you and it is archived within the *2017_rainfall_data* folder. For the sake of simplicity, we'll just be looking at data from 2017.\n",
    "\n",
    "## A few ground rules\n",
    "\n",
    " - Remember to run every cell.\n",
    "     - Parts of this workshop won't work if this condition isn't met.\n",
    " - Please don't change my asserts.\n",
    "     - If you're receiving an incorrect answer please don't change the assert answer just to get it right. You            won't learn anything and will probably fail the rest of the tutorial. Feel free to message slack on the #workshops channel if you get stuck.\n",
    "     \n",
    "Lets begin by importing some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't mind this. I'm just trying to double check you work :)\n",
    "def assertAns(condition, fail_str, suc_str):\n",
    "    assert condition, fail_str\n",
    "    print(suc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Linear algebra\n",
    "import requests as req #Python's http library\n",
    "import re #Python's Regex library\n",
    "import pandas as pd #Python's data manipulation library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost everything you do in computer science requires a datastructure to store information. Since each dataset consumes a lot of memory to load, many ill-rested academics have invented the dataframe to hold vast amounts of data.\n",
    "\n",
    "Dataframes are basically tables. Each dataframe is comprised of columns, rows, and a header.\n",
    "\n",
    "Yep sounds like a table. The reason why a dataframe resembles a table is because tables are very structured and easy to understand. If you look at a table, all cells underneath a header are part of the same \"attribute\". This standardization allows tables to store information in a neat manner.\n",
    "\n",
    "Lets go ahead and make our first dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to make your first dataframe!\n",
    "\n",
    "a2DArray = [[\"apple\",\"potato\"],\n",
    "            [\"banana\",\"onion\"]]\n",
    "\n",
    "\n",
    "myFirstDataframe = pd.DataFrame(a2DArray,columns=[\"fruits\",\"vegitables\"])\n",
    "#The following variable is the last variable that is returned in the cell\n",
    "myFirstDataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    " - The *columns* attribute in the *pd.DataFrame()* command specifies the headers for the dataframe\n",
    " - The numbers on the left hand side of the dataframe are the row indexes. Note how they were automatically generated.\n",
    " - Jupyter automatically displays the dataframe if the dataframe LAST VARIABLE to be RETURNED in the cell\n",
    " - We pass in a 2D array to make the dataframe because tables (and subsiquently dataframes) are basically 2D arrays!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a better understanding of the structure of a dataframe, lets import our dataset into a dataframe!\n",
    "\n",
    "Since CSV files are meant to be read by excel (which converts data into a table), it is easy for a dataframe to represent data originating from a CSV file.\n",
    "\n",
    "In fact, transforming data from a CSV file into a dataframe is so easy there is a built in function in Pandas that can turn a CSV file into a dataframe in one step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note how I am navigating through our file system to find the csv file I want.\n",
    "#The directory you start in is always in relation to the directory of the current notebook\n",
    "rainfallDF = pd.read_csv(\"../2017_rainfall_data/rainfall201706.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets don't always come in CSV files! If you find a dataset that came as a JSON file or even a a shapefile, don't hesitate to ask how to import these files on Slack!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the first five rows of our dataframe using the `head(n)` command. By default `n=5` so if you use `dataframe.head()`, it will return the first 5 elements of our dataframe. You'll mainly be using this command to have a glance at your data. It's pretty useful.\n",
    "\n",
    "Try that with our `rainfallDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfallDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sometimes we want to select a certain index (row) of a dataframe. To do so, we simply put `iloc[n]` before our dataframe:\n",
    "\n",
    "`dataframe.iloc[n]`\n",
    "\n",
    "Much like arrays, we replace _n_ with the index that we want to retrieve.\n",
    "\n",
    "Try to grab the first row of our dataframe using the `iloc[]` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfallRow.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets select a certain column within our dataframe!\n",
    "we do so like this:\n",
    "\n",
    "`newVarToHoldColumnVals = DF[\"theHeaderForTheColumn\"]`\n",
    "\n",
    "Try to make a variable to hold all the dates of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datesColumn = <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Data Science Process\n",
    "\n",
    "Whenever we're working with a dataset we usually go through 4 stages:\n",
    "\n",
    "1. Data Inspection\n",
    "2. Normalization and cleaning\n",
    "3. Gathering insights (Feature Engineering)\n",
    "4. Generate the models\n",
    "5. Storytelling\n",
    "\n",
    "### Data Inspection\n",
    "Every good data science project begins with data inspection. When we inspect the data we want to look at the beast and see what's coming. The point of inspecting the data is to gather context on what we're looking at so we can begin to ask ourselves questions to answer!\n",
    "\n",
    "Some things I usually look at when I'm glancing at data are the _means_ of the columns, the _unique values_ of categorical variables, _counts_, _max_, _mins_, and _modes_. This should spark some questions if you see an abnormally high number in a column.\n",
    "\n",
    "### Normalization and Cleaning\n",
    "Once we understand our data, we'll have to start cleaning our data for modeling. This involves clearing rows with null values (or imputing them with averages), removing useless columns (or columns that are too similiar to others), placing all values into a linear scale, or converting all numbers to a universal unit for your dataset.\n",
    "\n",
    "\n",
    "### Gathering Insights\n",
    "\n",
    "Before I discuss about insight gathering you need to understand these terms:\n",
    "\n",
    "__Data__ are values that describes something. Data is usually gathered from sensors or events that happen on your phone/computer (like a tap).\n",
    "\n",
    "__Information__ is data that is useful. Data is not meaningful. Information is.\n",
    "\n",
    "__Attributes__ are the variables of your dataset. If you have a dataset that describes a windows, the attributes of the dataset are the dimensions of the window, the types of material that it is made from. Think of attributes as the headers of your dataset. In a dataframe, the values that describes the attributes are the data underneath the headers.\n",
    "\n",
    "__Features__ are attributes that are useful. Attributes are not meaningful. Features are. The moment that an attribute tells us something about the dataset is the moment that it becomes a feature. Features are the useful bits of information that are fed into machine learning algorithms. Attributes can't be fed into machine learning algorithms because they don't tell the algorithm anything. It's just a jumble of numbers.\n",
    "\n",
    "__Feature Engineering__ is the extraction of meaningful information out of attributes in our dataset. Think of it as turning attributes into features. An example of feature engineering is determining the day of the week given a date. We are creating a new feature (the day of the week) given an attribute (the date). Another example is extracting the surname out of a full name. This is considered feature engineering because surnames can identify people who are part of the same family in a dataset.\n",
    "\n",
    "With our data cleaned and normalized we feature engineer to discover more patterns and insights within our dataset. The goal of feature engineering is to provide more variables for our algorithms to play with. By manually identifying these patterns, the computer doesn't have to expend additional effort to discover these patterns algorithmically.\n",
    "\n",
    "This is the hardest part of data science so don't worry if you need more explination or aren't good at it. Feel free to message @curtischong5 on Slack if you want further explination!\n",
    "\n",
    "\n",
    "\n",
    "From experience, steps 1-3 should take up 90% of your time. It also just so happens that the first three steps occur organically with the inception of your questions. Once you start looking at the data, you'll want to answer some questions... which will lead you to clean the data to gather insights from it. Then you may have more questions so you'll repeat the cycle.\n",
    "\n",
    "### Generating the models\n",
    "Once our dataset has been prepared (and you've answered most of your questions), we can finally have some fun and do some machine learning to make predictions! Depending on the type of dataset, you may want to implement a regression algorithm to predict a numeric value (like predicting the speed of a car given these variables) or sort variables into different categories (like passing or failing a test). We'll dive deeper into machine learning in workshop #3.\n",
    "\n",
    "### Storytelling\n",
    "The purpose of data science is to tell a story. We crunch numbers to discover new things and to propose new policies. Our world will crumble if decisions were not backed by data. So whatever you do when you're analyzing your dataset(s) always keep this purpose in the back of your mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets inspect our data. One useful method to learn about the specifics of your dataset is by running the `describe()` function on our `rainfallDF`. So lets try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfallDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: std means standard deviation and the numbers at the 25%, 50%, and 75% mark are numeric percentiles of the quartile ranges._\n",
    "\n",
    "That interesting. Notice how the number of rainfall \"data\" doesn't match the number of *ID*s. This doesn't make sense as every id must have a corresponding *rainfall* value. I suspect that some missing data is in the rainfall column. Lets run the `dropna()` function to try to remove all rows that are missing data or contains \"NaN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droppedNa = rainfallDF.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets re-run the `describe()` function on our dataframe to see if the number of *ID*s and *rainfall* are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droppedNa.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm a bit curious... What is the average amount of rainfall that usually falls? Assign that number into the *averageRainfall* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageRainfall = <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertAns(averageRainfall == 0.012369, \"That is not the average rainfall!\",\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A bit more about the methodology we just went through. Notice how we asked ourselves some questions after inspecting the data. Next, we had to clean our dataset to answer our question. Finally, we asked ourselves more questions about the data. We kept jumping back and forth on the four stages! Since a large part of data science falls within the realm of data exploration, it is hard to estimate how long it'll take to analyze data. So start early on your projects and give yourself a deadline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets practice selecting columns again. Try to select the *rainfall* column below and save it into the dataframe `rainfallColumn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfallColumn = rainfallDF[<FILL IN>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using this column, lets try to run the `mean()` function on it. This will give us the average amount of rainfall that fell. We're going to save this value into the `avgRainfall` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgRainfall = rainfallColumn.<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(avgRainfall == 0.012369, \"That is not the average rainfall!\", \"Test Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also run the `sum()` function on the `rainfallColumn` to determine how much rain had fell on Toronto in the first few months of 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgRainfall = rainfallColumn.<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertAns(avgRainfall == 4297.2639990000007, \"That is not the total rainfall!\",\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `averageRainFall` is the same value as the value we were told from the *describe()* function.\n",
    "\n",
    "I'm going to showcase one last function: `unique()`. This function will display all unique values in a column which makes it really useful for categorical columns. More information about categorical variables here: http://www.stat.yale.edu/Courses/1997-98/101/catdat.htm.\n",
    "\n",
    "Lets run `unique()` on the \"name\" column of the `rainfallDF`. We'll save this value to the `uniqueNames` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueNames = rainfallDF[<FILL IN>].<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertAns(list(uniqueNames) == ['RG_001', 'RG_002', 'RG_003', 'RG_004', 'RG_006', 'RG_007','RG_012', 'RG_013', 'RG_014', 'RG_015', 'RG_016', 'RG_017','RG_018', 'RG_019', 'RG_020', 'RG_021', 'RG_022', 'RG_023','RG_024', 'RG_025', 'RG_027', 'RG_028', 'RG_030', 'RG_031','RG_033', 'RG_034', 'RG_035', 'RG_036', 'RG_037', 'RG_038','RG_040', 'RG_041', 'RG_042', 'RG_044', 'RG_045', 'RG_046','RG_047', 'RG_048', 'RG_049', 'RG_051', 'RG_052', 'RG_054','RG_055', 'RG_056'], \"Those aren't the unique station names!\",\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These small functions (`sum()`, `mean()`, `unique()`), and others like it (`mode()`, `median()`, etc.) are great for glancing at your dataset. They allow you to take a look and learn how data varies for individual columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we end this workshop, I would like to draw attention to one of the more important aspects of data analytics... learning how to generate features from existing attributes. In other words, I'll give you the tool to assist you with the third stage. This time, I'll lead by example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfallDF[\"date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm a curious and want to know what is the average rainfall for every hour of the day.\n",
    "#I'll begin by looping through each row in the dataframe and take the \"time\" within the\n",
    "#date attribute and put it into another column called time.\n",
    "transformedDF = rainfallDF #Lets make a copy of the existing dataframe.\n",
    "for index, row in transformedDF.iterrows():\n",
    "    dateForThisRow = row[\"date\"]\n",
    "    theTime = dateForThisRow.split(\"T\")[1] #values in the date column looks like this: 2017-06-01T00:00:00\n",
    "    #I am spliting the string on the \"T\" and selecting the \"1\" index because\n",
    "    #that was the time when the value was taken.\n",
    "    rainfallDF.set_value(index,'time', theTime) #Use this command to set the values of a new column.\n",
    "#Lets have a quick look at the transformed dataframe:\n",
    "transformedDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedDF[\"time\"].unique() #Wow. It seems like they take measurements every 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With this new dataframe I want to create an array that describes the average amount of rainfall for every hour.\n",
    "#I'll start off by selecting just the first minute of the first hour of the day.\n",
    "firstMinuteDF = transformedDF[transformedDF[\"time\"] == \"00:00:00\"]\n",
    "firstMinuteDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets manipulate the DF again and add a new \"hour\" column.\n",
    "transformedDF2 = transformedDF #Lets make a copy of the existing dataframe.\n",
    "#We can achieve this by looping through every row of our dataframe and extracting.\n",
    "#The \"hour\" value from the date attribute.\n",
    "for index, row in transformedDF2.iterrows():\n",
    "    timeForThisRow = row[\"time\"]\n",
    "    theHour = timeForThisRow.split(\":\")[0] #Values in the \"time\" column looks like this: 00:00:00\n",
    "    #I am spliting the string on the \":\" and selecting the \"0\" index because that\n",
    "    #is the hour when the value was taken.\n",
    "    rainfallDF.set_value(index,'hour', theHour) #Use this command to set the values of a new column.\n",
    "#lets have a quick look at the transformed dataframe:\n",
    "transformedDF2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to teach you some new selecting commands. Watch how I am selecting all rows that were measured from the first hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstHourDF = transformedDF2[transformedDF2[\"hour\"] == \"00\"] #I'm saying: select all rows\n",
    "#that has an \"hour\" value of \"00\".\n",
    "print(firstHourDF[\"rainfall\"].mean())\n",
    "#The following value is the average amount of rainfall for all measurements\n",
    "#that were taken within the first hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets try the same measurement for all hours of the day.\n",
    "arrToHoldHours = []\n",
    "for hour in range(23):\n",
    "    strHour = str(hour)\n",
    "    #Here I am formating the \"hour\" so it matches with the style of the hour values in the DF.\n",
    "    if len(strHour) == 1:\n",
    "        strHour = \"0\" + strHour\n",
    "    #Lets actually do the selecting now.\n",
    "    selectedHour = transformedDF2[transformedDF2[\"hour\"] == strHour]\n",
    "    avgRainfallForThatHour = selectedHour[\"rainfall\"].mean() #Calculating the mean.\n",
    "    arrToHoldHours.append(\"hour \" + strHour + \": \" + str(avgRainfallForThatHour))\n",
    "    \n",
    "#After all that work we can finally print the data.\n",
    "for oneHour in arrToHoldHours:\n",
    "    print(oneHour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most percipitation happens at 2AM. It might be interesting to see how measurements from different seasons differ and even from different years. (If you're interested download the other datasets https://goo.gl/gBYrb4 and merge them into one big dataframe!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before I let out the workshop, I'll teach you one last selection method; how to impose multiple rules when selecting rows. You may have noticed how I used `transformedDF[transformedDF[\"time\"] == \"00:00:00\"]` to select all measurements that were taken from the first minute of the day. But what if I want to select values that came from the first minute _AND_ from a specific station... say from *RG_001*. I'll have to use syntax to define multi-condition selection. Here's how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformedDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b49fcccbaa74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirstMinOfDayAndRG_001\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformedDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformedDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"00:00:00\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransformedDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RG_001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#Notice the parenthesis between each condition.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Amount selected: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstMinOfDayAndRG_001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfirstMinOfDayAndRG_001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformedDF' is not defined"
     ]
    }
   ],
   "source": [
    "firstMinOfDayAndRG_001 = transformedDF[(transformedDF[\"time\"] == \"00:00:00\") & (transformedDF[\"name\"] == \"RG_001\")]\n",
    "#Notice the parenthesis between each condition.\n",
    "print(\"Amount selected: \" + str(len(firstMinOfDayAndRG_001)))\n",
    "firstMinOfDayAndRG_001.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to save our work by saving the dataframe we made. (So we don't have to process everything all over again everytime we close Jupyter). The _pickle_ file type can store Python dataframes as a _.pkl_ file. To save our dataframe, run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedDF.to_pickle(\"../saved_dataframes/workshop1RainfallDF\")\n",
    "#Note how we're saving our pickle file into the saved_dataframes folder\n",
    "#Keeping all your data in one place will keep you organised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Congrats for making it this far. I hope this tutorial was helpful and please don't hesitate to ask for help. Feedback that you provide will be taken into consideration for future workshops and I hope that you learned something. Good luck on your projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
